{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIKM 2021 Tutorial Notebook - Part 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UxEkLc6yz6J"
      },
      "source": [
        "# PyTerrier CIKM Tutorial Notebook - Part 1\n",
        "\n",
        "This is one of a series of Colab notebooks created for the [CIKM 2021](https://www.cikm2021.org/) Tutorial entitled '**IR From Bag-of-words to BERT and Beyond through Practical Experiment**'. It demonstrates the use of [PyTerrier](https://github.com/terrier-org/pyterrier) on the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
        "\n",
        "In particular, this notebooks has the following learning outcomes:\n",
        "  - PyTerrier installation & configuration\n",
        "  - indexing a collection\n",
        "  - accessing an index\n",
        "  - using the `BatchRetrieve` transformer for searching an index\n",
        "  - conducting an `Experiment` \n",
        "\n",
        "Pre-requisites:\n",
        " - We assume that you are confident in programming Python, including [lambda functions](https://www.w3schools.com/python/python_lambda.asp).\n",
        " - We will **only be supporting notebooks on the Google Colab platform**.\n",
        "  > *Explanation*: You can try this notebook on Linux/Mac/Windows, but we have verified these notebooks on Google Colab for the purposes of this tutorial.\n",
        "\n",
        "Related Reading:\n",
        " - [Pandas documentation](https://pandas.pydata.org/docs/)\n",
        " - [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u2hD-zBzfpR"
      },
      "source": [
        "PyTerrier is a Python framework, but uses the underlying [Terrier information retrieval toolkit](http://terrier.org) for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations. \n",
        "\n",
        "In the following, we introduce everything you need to know about PyTerrier, and also provide appropriate links to relevant parts of the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH0Ds2370V0G"
      },
      "source": [
        "### Installation & Configuration\n",
        "\n",
        "PyTerrier is installed as follows. This might take a few minutes, so you can read on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oE5neAX0bkW"
      },
      "source": [
        "!pip install -q python-terrier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUN1B8RI0gPC"
      },
      "source": [
        "The next step is to initialise PyTerrier. This is performed using PyTerrier's `init()` method. The `init()` method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent `init()` from being called more than once by checking `started()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4qALBa90-7g"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qqjVSu-5_FX"
      },
      "source": [
        "### Documents, Indexing and Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3soS1IIy5B83"
      },
      "source": [
        "Much of PyTerrier's view of the world is wrapped up in Pandas dataframes. Let's consider some textual documents in a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSEiEuTE5uyL"
      },
      "source": [
        "# we need to import pandas. We commonly rename it to pd, to make commands shorter\n",
        "import pandas as pd\n",
        "\n",
        "# lets not truncate output too much\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "\n",
        "docs_df = pd.DataFrame([\n",
        "        [\"d1\", \"this is the first document of many documents\"],\n",
        "        [\"d2\", \"this is another document\"],\n",
        "        [\"d3\", \"the topic of this document is unknown\"]\n",
        "    ], columns=[\"docno\", \"text\"])\n",
        "\n",
        "docs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RCtCCTU6GAj"
      },
      "source": [
        "Before any search engine can estimate which documents are most likely to be relevant for a given query, it must index the documents. \n",
        "\n",
        "In the following cell, we index the dataframe's documents. The index, with all its data structures, is written into a directory called `index_3docs`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YvLhEOS6V8w"
      },
      "source": [
        "indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True)\n",
        "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
        "index_ref.toString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUm6r6_625gW"
      },
      "source": [
        "An `IndexRef`\n",
        " is essentially a string saying where an index is stored. Indeed, we can look in the `index_3docs` directory and see that it has created various small files: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF45pl5O8p7R"
      },
      "source": [
        "!ls -lh index_3docs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2b8isFP3Kv6"
      },
      "source": [
        "With an `IndexRef`, we can load it to an actual index. The method `pt.IndexFactory.of()` is the relevant factory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTM17szD6pNy"
      },
      "source": [
        "index = pt.IndexFactory.of(index_ref)\n",
        "\n",
        "#lets see what type index is.\n",
        "type(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZe3HD5i7G3v"
      },
      "source": [
        "Ok, so this object refers to Terrier's [`Index`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html) type. Check the linked Javadoc – you will see that this Java object has methods such as:\n",
        " - `getCollectionStatistics()`\n",
        " - `getInvertedIndex()`\n",
        " - `getLexicon()`\n",
        "\n",
        "Let's see what is returned by the `CollectionStatistics()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-gXEDSX65bx"
      },
      "source": [
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6HrR4lc7i10"
      },
      "source": [
        "Ok, that seems fair – we have 3 documents. But why only 4 terms? \n",
        "Let's check the [`Lexicon`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Lexicon.html), which is our vocabulary. Fortunately, the `Lexicon` can be iterated easily from Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us2mAzTW7Bny"
      },
      "source": [
        "for kv in index.getLexicon():\n",
        "  print(\"%s (%s) -> %s (%s)\" % (kv.getKey(), type(kv.getKey()), kv.getValue().toString(), type(kv.getValue()) ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwbp94gh86pw"
      },
      "source": [
        "Here, iterating over the `Lexicon` returns a pair of `String ` term and a [`LexiconEntry`](http://terrier.org/docs/current/javadoc/org/terrier/structures/LexiconEntry.html) object – which itself is an [`EntryStatistics`](http://terrier.org/docs/current/javadoc/org/terrier/structures/EntryStatistics.html) – and contains information including the statistics of that term.\n",
        "\n",
        "\n",
        "So what did we find? Here are some observations:\n",
        " - we only have 4 unique terms, as stopwords were removed;\n",
        " - we have one term for `\"document\"`, even though `\"documents\"` occurred in document \"`d1`\". \n",
        " \n",
        "Both these observations make sense, as indeed Terrier removes standard stopwords and applies Porter's stemmer by default.\n",
        "\n",
        "Further:\n",
        " - `Nt` is the number of unique documents that each term occurs in – this is useful for calculating IDF.\n",
        " - `TF` is the total number of occurrences – some weighting models use this instead of Nt.\n",
        " - The numbers in the `@{}` are a pointer – they tell Terrier where the postings are for that term in the inverted index data structure.\n",
        "\n",
        "Finally, we can also use the square bracket notation to lookup terms in Terrier's lexicon:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZmi9498-Ijw"
      },
      "source": [
        "index.getLexicon()[\"document\"].toString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaKaU59l-kzg"
      },
      "source": [
        "Let's now think about the inverted index. Remember that the inverted index tells us in which *documents* each term occurs in. The `LexiconEntry` is the pointer that tell us where to find the postings for that term in the inverted index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQki_Pds8ut2"
      },
      "source": [
        "pointer = index.getLexicon()[\"document\"]\n",
        "for posting in index.getInvertedIndex().getPostings(pointer):\n",
        "    print(posting.toString() + \" doclen=%d\" % posting.getDocumentLength())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EaoIIO_DPx"
      },
      "source": [
        "Ok, so we can see that `\"document\"` occurs once in each of the three documents. \n",
        "\n",
        "NB: Terrier counts documents as integers from 0 (called *docids*). It records the mapping back to *docnos* (the string form, i.e. \"`d1`\", \"`d2`\") in a separate data structure called the *metaindex*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOSdVAr-CGRf"
      },
      "source": [
        "### Searching an Index\n",
        "\n",
        "Our way into search in PyTerrier is called `BatchRetrieve`. BatchRetrieve is configured by specifying an index and a weighting model (`Tf` in our example). We then search for a single-word query, `\"document\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtK93nwXCF5C"
      },
      "source": [
        "br = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
        "br.search(\"document\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHqSfTCtDM2T"
      },
      "source": [
        "So the `search()` method returns a dataframe with columns:\n",
        " - `qid`: this is by default \"1\", since it's our first and only query\n",
        " - `docid`: Terrier' internal integer for each document\n",
        " - `docno`: the external (string) unique identifier for each document\n",
        " - `score`: since we use the `Tf` weighting model, this score corresponds the total frequency of the query (terms) in each document\n",
        " - `rank`: A handy attribute showing the descending order by score\n",
        " - `query`: the input query\n",
        "\n",
        "As expected, the `Tf` weighting model used here only counts the frequencies of the query terms in each document, i.e.:\n",
        "$$\n",
        "score(d,q) = \\sum_{t \\in q} tf_{t,d}\n",
        "$$\n",
        "\n",
        "Hence, it's clear that document `d1` should be the highest scored document with two occurrences (c.f. `'document'` and `'documents'`).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBXquPOD6q7"
      },
      "source": [
        "We can also pass a dataframe of one or more queries to the `transform()` method (rather than the `search()` method) of a transformer, with queries numbered \"q1\", \"q2\" etc.. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPBmPOETBKWk"
      },
      "source": [
        "import pandas as pd\n",
        "queries = pd.DataFrame([[\"q1\", \"document\"], [\"q2\", \"first document\"]], columns=[\"qid\", \"query\"])\n",
        "br.transform(queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcgDzFLBEWAI"
      },
      "source": [
        "In fact, we are usually calling `transform()`, so it's the default method – i.e. \n",
        "`br.transform(queries)` can be more succinctly written as `br(queries)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCwxb3HhEOp_"
      },
      "source": [
        "br(queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldY8VV8wQ60Z"
      },
      "source": [
        "### CORD19\n",
        "\n",
        "OK, having 3 documents is quite trivial, so let's move upto a slightly larger corpus of documents. We'll be using the CORD19 datasets for the remainder of this tutorial. PyTerrier has a handy `get_dataset()` API, which allows us to download the corpus and index it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2lJsK-vEcQx"
      },
      "source": [
        "import os\n",
        "\n",
        "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "pt_index_path = './terrier_cord19'\n",
        "\n",
        "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
        "  # create the index, using the IterDictIndexer indexer \n",
        "  indexer = pt.index.IterDictIndexer(pt_index_path, meta_reverse=[])\n",
        "\n",
        "  # we give the dataset get_corpus_iter() directly to the indexer\n",
        "  # while specifying the fields to index and the metadata to record\n",
        "  index_ref = indexer.index(cord19.get_corpus_iter(), \n",
        "                            fields=('abstract',), \n",
        "                            meta=('docno',))\n",
        "\n",
        "else:\n",
        "  # if you already have the index, use it.\n",
        "  index_ref = pt_index_path\n",
        "\n",
        "index = pt.IndexFactory.of(index_ref)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7GK9uANRt8w"
      },
      "source": [
        "#### Task 1\n",
        "- What are the statistics of our index?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNAVqf9uRr2p"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9hmeECHbjC5R"
      },
      "source": [
        "#@title View Solution\n",
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQD9Q8CqSirN"
      },
      "source": [
        "Next, CORD19 also has a corresponding set of queries and relevance assessments (aka qrels), thus forming a *test collection*, \n",
        "\n",
        "We can easily access the topics and qrels from the dataset. Indeed these are expressed as dataframes as well (we use Pandas's [`head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method to show only the first 5 topics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n7oY-YYS_-A"
      },
      "source": [
        "cord19.get_topics(variant='title').head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rYxqvhJTGNX"
      },
      "source": [
        "cord19.get_qrels().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gop4-jVbIIu"
      },
      "source": [
        "### Weighting Models\n",
        "\n",
        "So far, we have been using the simple \"`Tf`\" as our ranking function for document retrieval in BatchRetrieve. However, we can use other models such as `\"TF_IDF\"` by simply changing the `wmodel=\"Tf\"` keyword argument in the constructor of `BatchRetrieve`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg8AGzCibdPG"
      },
      "source": [
        "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "tfidf.search(\"chemical reactions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6aZGX9sbdmc"
      },
      "source": [
        "You will note that, as expected, the scores of documents ranked by `TF_IDF` are no longer integers. You can see the exact formula used by Terrier from [the Github repo](https://github.com/terrier-org/terrier-core/blob/5.x/modules/core/src/main/java/org/terrier/matching/models/TF_IDF.java#L79).\n",
        "\n",
        "Terrier supports many weighting models – the documentation contains [a list of supported models](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html) - some of which we will discover later in the tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ0j9lFfx-gO"
      },
      "source": [
        "### What is Success?\n",
        "\n",
        "So far, we have been creating search engine models, but we haven't decided if any of them are actually any good. Let's investigate if we are getting a correct (\"relevant\") document at the first rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyShZYpwwNSx"
      },
      "source": [
        "qrels = cord19.get_qrels()\n",
        "def get_res_with_labels(ranker, df):\n",
        "  # get the results for the query or queries\n",
        "  results = ranker( df )\n",
        "  # left outer join with the qrels\n",
        "  with_labels = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n",
        "  return with_labels\n",
        "\n",
        "# lets get the Tf results for the first query\n",
        "get_res_with_labels(tfidf, cord19.get_topics(variant='title').head(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFUmiFSobUDg"
      },
      "source": [
        "from pyterrier.measures import *\n",
        "\n",
        "pt.Experiment(\n",
        "    [tfidf],\n",
        "    cord19.get_topics(variant='title'),\n",
        "    cord19.get_qrels(),\n",
        "    eval_metrics=[\"map\", \"ndcg\", nDCG@10])\n",
        "# we can trec_eval (string) measure names, e.g. \"map\", \n",
        "# or the irmeasures syntax, e.g nDCG@10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmTmuwd2mVmL"
      },
      "source": [
        "# Prebuilt Index from Terrier Data Repository\n",
        "\n",
        "In the rest of this tutorial, we'll be using a provided index from the [Terrier Data Repository](http://data.terrier.org/). You can view all of the index variants available for the TREC Covid test collection at: http://data.terrier.org/trec-covid.dataset.html. We will use the standard 'terrier_stemmed' index variant. \n",
        "\n",
        "You will see progress bars while the files are downloading. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ty0MmPmVSk"
      },
      "source": [
        "bm25 = pt.BatchRetrieve.from_dataset(\n",
        "    'trec-covid', \n",
        "    'terrier_stemmed', \n",
        "    wmodel='BM25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5q7dLtkn7KB"
      },
      "source": [
        "Lets see how BM25 compares to TF_IDF - how many queries have improved (M)AP?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd0UfrDio8jz"
      },
      "source": [
        "#YOUR SOLUTION HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fv4Dxu8tn1_K"
      },
      "source": [
        "#@title See Solution\n",
        "pt.Experiment( \n",
        "    [tfidf, bm25],\n",
        "    cord19.get_topics(variant='title'),\n",
        "    cord19.get_qrels(),\n",
        "    baseline=0,\n",
        "    eval_metrics=[\"map\", \"ndcg\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmBP6Nd9xzqJ"
      },
      "source": [
        "You should see that 40 queries exhibited improvements in AP when using BM25, while 10 are degraded; This is a significant increase ($p < 0.05$). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt0iPhRw2J-S"
      },
      "source": [
        "## That's all folks\n",
        "\n",
        "The following parts of the PyTerrier documentation may be useful references for this notebook:\n",
        " * [PyTerrier datasets](https://pyterrier.readthedocs.io/en/latest/datasets.html)\n",
        " * [Using Terrier for retrieval](https://pyterrier.readthedocs.io/en/latest/terrier-indexing.html)\n",
        " * [Transformers in PyTerrier](https://pyterrier.readthedocs.io/en/latest/transformer.html)\n",
        " * [Transformer Operators](https://pyterrier.readthedocs.io/en/latest/operators.html)\n",
        "\n",
        "If you arent coming back to Part 2 of the tutorial, please dont forget to complete our exit quiz: https://forms.office.com/r/RiYSAxAKhk"
      ]
    }
  ]
}